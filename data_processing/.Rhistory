set.seed(4)
train_id = sample(1:dim(Boston)[1], dim(Boston)[1]/2)
test_id = -train_id
B_train = Boston[train_id,]
B_test = Boston[test_id,]
err = numeric(length=16)
for( i in (3:16)){
lm = lm(nox~ bs(dis, df=i), data = B_train)
pred = predict(lm, newdata = B_test)
err[i] = mean((pred-B_test$nox)^2)
}
plot(3:16, err[3:16], main='degree vs err', type = 'l')
which(err[3:16]==min(err[3:16]))
library(ISLR)
data(OJ)
set.seed(15)
train_id = sample(1:dim(OJ)[1], 800)
test_id = -train_id
OJ_train = OJ[train_id,]
OJ_test = OJ[test_id,]
length(test_id)
length(OJ_test)
length(OJ_train)
dim(OJ_train)
dim(OJ_test)
set.seed(11)
train_id = sample(1:dim(College)[1], dim(College)[1]/2)
test_id = -train_id
College_train = College[train_id,]
College_test = College[test_id,]
dim(College_train)
dim(College_test)
library(tree)
tree1 = tree(Purchase~., data =OJ_train)
summary(tree1)
tree1
plot(tree1)
text(tree1, pretty=0)
tree.pred = predict(tree1,OJ_test, type='class')
table(tree.pred,OJ_test$Purchase)
Er_rate=(22+39)/(dim(OJ_test)[1])
Er_rate
set.seed(3)
cv.tree1 = cv.tree(tree1,FUN=prune.misclass)
names(cv.tree1)
cv.tree1$dev
cv.tree1$size
plot(cv.tree1$size,cv.tree1$dev,type='b')
prune.tree1=prune.tree(tree1,best=4)
plot(prune.tree1)
text(prune.tree1,pretty=0)
summary(prune.tree1)
treep.pred = predict(prune.tree1,OJ_test, type='class')
table(treep.pred,OJ_test$Purchase)
Er_rate=(16+49)/(dim(OJ_test)[1])
Er_rate
x1=c(3,2,4,1,2,4,4)
x2=c(4,2,4,4,1,3,1)
plot(x1,x2, col = c("red", "red", "red", "red", "blue","blue","blue"))
plot(x1,x2,col = c("red", "red", "red", "red", "blue","blue","blue"))
abline(-0.5,1)
abline(0,1, lty=3)
abline(-1,1,lty=3)
1
abline(-0.1,1, col= 'green')
= '
points(2.5,1, col = 'red')
set.seed(10)
x1 = rnorm(100)
x2 = rnorm(100)
ep = rnorm(100, sd =0.1)
y = 2+2*x1+2*x2+ep
beta1 = 10
a=y-beta1*x1
beta2=lm(a~x2)$coef[2]
beta2
a=y-beta2*x2
beta1=lm(a~x1)$coef[2]
beta1
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 10
for(i in 1:1000){
a = y - beta1[i]*x1
beta2[i] = lm(a~x2)$coef[2]
a=y-beta2[i]*x2
lm.fit = lm(a~x1)
if(i<1000){
beta1[i-1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
lm1 = lm(y~x1+x2)
summary(lm1)
points(1:1000, lm1$coefficients[1]*rep(1,1000), col = 'orange')
points(1:1000, lm1$coefficients[2]*rep(1,1000), col = 'purple')
points(1:1000, lm1$coefficients[3]*rep(1,1000), col = 'gray')
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 10
for(i in 1:1000){
a = y - beta1[i]*x1
beta2[i] = lm(a~x2)$coef[2]
a=y-beta2[i]*x2
lm.fit = lm(a~x1)
if(i<1000){
beta1[i-1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 5
for(i in 1:1000){
a = y - beta1[i]*x1
beta2[i] = lm(a~x2)$coef[2]
a=y-beta2[i]*x2
lm.fit = lm(a~x1)
if(i<1000){
beta1[i-1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
#Generate data
set.seed(10)
x1 = rnorm(100)
x2 = rnorm(100)
ep = rnorm(100, sd =0.1)
y = 1+2*x1+2*x2+ep
beta1 = 10
a=y-beta1*x1
beta2=lm(a~x2)$coef[2]
a=y-beta2*x2
beta1=lm(a~x1)$coef[2]
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 5
for(i in 1:1000){
a = y - beta1[i]*x1
beta2[i] = lm(a~x2)$coef[2]
a=y-beta2[i]*x2
lm.fit = lm(a~x1)
if(i<1000){
beta1[i-1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
lm1 = lm(y~x1+x2)
set.seed(10)
x1 = rnorm(100)
x2 = rnorm(100)
ep = rnorm(100, sd =0.1)
y = 2+2*x1+2*x2+ep
beta1 = 5
a=y-beta1*x1
beta2=lm(a~x2)$coef[2]
a=y-beta2*x2
beta1=lm(a~x1)$coef[2]
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 5
for (i in 1:1000) {
a = Y - beta1[i] * X1
beta2[i] = lm(a ~ X2)$coef[2]
a = Y - beta2[i] * X2
lm.fit = lm(a ~ X1)
if (i < 1000) {
beta1[i + 1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
set.seed(10)
x1 = rnorm(100)
x2 = rnorm(100)
ep = rnorm(100, sd =0.1)
y = 1+2*x1+3*x2+ep
beta1 = 5
a=y-beta1*x1
beta2=lm(a~x2)$coef[2]
a=y-beta2*x2
beta1=lm(a~x1)$coef[2]
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 5
for (i in 1:1000) {
a = Y - beta1[i] * X1
beta2[i] = lm(a ~ X2)$coef[2]
a = Y - beta2[i] * X2
lm.fit = lm(a ~ X1)
if (i < 1000) {
beta1[i + 1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
set.seed(10)
x1 = rnorm(100)
x2 = rnorm(100)
ep = rnorm(100, sd =0.1)
y = 1+2*x1+3*x2+ep
beta1 = 5
a=y-beta1*x1
beta2=lm(a~x2)$coef[2]
a=y-beta2*x2
beta1=lm(a~x1)$coef[2]
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 5
for (i in 1:1000) {
a = Y - beta1[i] * X1
beta2[i] = lm(a ~ X2)$coef[2]
a = Y - beta2[i] * X2
lm.fit = lm(a ~ X1)
if (i < 1000) {
beta1[i + 1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
for (i in 1:1000) {
a = y - beta1[i] * X1
beta2[i] = lm(a ~ X2)$coef[2]
a = y - beta2[i] * X2
lm.fit = lm(a ~ X1)
if (i < 1000) {
beta1[i + 1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
for (i in 1:1000) {
a = y - beta1[i] * x1
beta2[i] = lm(a ~ x2)$coef[2]
a = y - beta2[i] * x2
lm.fit = lm(a ~ x1)
if (i < 1000) {
beta1[i + 1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
set.seed(10)
x1 = rnorm(100)
x2 = rnorm(100)
ep = rnorm(100, sd =0.1)
y = 2.5+2*x1+1.5*x2+ep
beta1 = 5
a=y-beta1*x1
beta2=lm(a~x2)$coef[2]
a=y-beta2*x2
beta1=lm(a~x1)$coef[2]
library(numbers)
beta0 = numeric(length =1000)
beta1 = numeric(length=1000)
beta2 = numeric(length=1000)
beta1[1] = 5
for (i in 1:1000) {
a = y - beta1[i] * x1
beta2[i] = lm(a ~ x2)$coef[2]
a = y - beta2[i] * x2
lm.fit = lm(a ~ x1)
if (i < 1000) {
beta1[i + 1] = lm.fit$coef[2]
}
beta0[i] = lm.fit$coef[1]
}
plot(1:1000, beta2, col = 'green')
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
plot(1:1000, beta2, col = 'green', ylim = c(-1, 3))
points(1:1000, beta1, col = 'blue')
points(1:1000, beta0, col = 'red')
lm1 = lm(y~x1+x2)
bsummary(lm1)
summary(lm1)
points(1:1000, lm1$coefficients[1]*rep(1,1000), col = 'orange')
points(1:1000, lm1$coefficients[2]*rep(1,1000), col = 'purple')
points(1:1000, lm1$coefficients[3]*rep(1,1000), col = 'gray')
pm1 = seq(from = 0, to = 1, by = 0.01)
pm1
gini = function(x){
y = x*(1-x) + (1-x)*x
return(y)
}
err_rate = function(x){
y = 1-pmax(x, 1-x)
return(y)
}
entropy = function(x){
y = -x*log(x)-(1-x)*log(1-x)
return(y)
}
curve(entropy, from = 0, to = 1, ylab = '', type = 'l', col = 'blue', ylim =c(0,0.8))
par(new=T)
curve(err_rate, from = 0, to = 1, ylab = '', type = 'l', col = 'orange', ylim = c(0,0.8))
par(new =T)
curve(gini, from=0, to= 1, ylab='function', type = 'l', col = 'green', ylim = c(0,0.8))
library(ISLR)
data("Hitters")
Hitters
which(Hitters$Salary != 'NA')
H_data = Hitters[which(Hitters$Salary != 'NA'),]
H_data$Salary = log(H_data$Salary)
set.seed(45)
H_train = H_data[1:200,]
H_test = H_data[201:dim(H_data)[1],]
dim(H_test)
library(gbm)
set.seed(103)
pows = seq(-10, -0.2, by = 0.1)
shrink = 10^pows
MSE = numeric(length= length(shrink))
MSE_test = numeric(length= length(shrink))
for(i in 1:length(shrink)){
boost.salary = gbm(Salary ~ ., data = H_train, distribution = "gaussian",
n.trees = 1000, shrinkage = shrink[i])
train.pred = predict(boost.salary, H_train, n.trees = 1000)
test.pred = predict(boost.salary, H_test, n.trees = 1000)
MSE[i] = mean((H_train$Salary-train.pred)^2)
MSE_test[i] = mean((H_test$Salary-test.pred)^2)
}
plot(shrink, MSE, type = 'l', main = 'Train MSE', col = 'red')
points(shrink, MSE_test, type = 'l', main = 'Test MSE', col = 'blue')
plot(shrink, MSE, type = 'l', main = 'Train MSE', col = 'red')
points(shrink, MSE_test, type = 'l', main = 'Test MSE', col = 'blue')
lm1 = lm(Salary~., data = H_train)
lm1.pred = predict(lm1, H_test)
MSE_lm1 = mean((H_test$Salary - lm1.pred)^2)
MSE_lm1
x_train = model.matrix(Salary~.,data = H_train) #-1 removes intercept row
x_test = model.matrix(Salary ~ ., data = H_test)
grid=10^seq(10,-2,length=100)
ridge.mod = glmnet(x_train,H_train$Salary, alpha = 0, lambda = grid, thresh = 1e-12)
plot(ridge.mod)
cv.out = cv.glmnet(x_train, H_train$Salary, alpha = 0, lambda = grid, thresh = 1e-12)
plot(cv.out)
bestlam=cv.out$lambda.min
ridge.pred=predict(ridge.mod, s = bestlam, newx = x_test)
ridge_err = mean((ridge.pred - H_test$Salary)^2)
ridge_err
bestlam
summary(lm1)
library(randomForest)
set.seed(4)
bag.salary = randomForest(Salary~.,data=H_train, mtry = 19, importance = TRUE)
bag.salary
yhat_bag = predict(bag.salary, newdata = H_test)
plot(yhat_bag, H_test$Salary)
mean((yhat_bag-H_test$Salary)^2)
radius = 2
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), xlab = "X1",
ylab = "X2")
symbols(c(-1), c(2), circles = c(radius), add = TRUE, inches = FALSE)
#Write the basis functions
b1 = function(x){
y = 0
if (0<x && x<=2){
y = 1}
if (x >=1 && x <=2){
y = 1 - (x-1)
}
return(y)
}
b2 = function(x){
y = 0
if(3<=x && x<=4){
y = x-3
}
if(x>4 && x<=5){
y = 1
}
return(y)
}
rf = function(x){
y = 1 + b1(x)+3*b2(x)
return(y)
}
x1 = seq(-2,2,0.1)
#We need to preallocate the vector we are writing to in the loop
out <- numeric(length = length(x1))
for(i in 1:length(x1)){
out[i] = rf(x1[i])
}
plot(x1, out)
library(tensorflow)
install.packages(tensorflow)
install_tensorflow(version = "gpu")
devtools::install_github("rstudio/tensorflow")
install.packages(devtools)
install.packages("devtools")
devtools::install_github('rstudio\tensorflow')
install.packages('Rtools')
install.packages('installr')
updateR()
require(installr)
updateR()
reuqire(devtools)
require(devtools)
require('devtools')
install.packages('devtools')
require(devtools)
require(devtools)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow()
sess = tf$Session
hello <- tf$constant('Hello, Tensorflow!')
sess$run(hello)
sess = tf$Session()
hello <- tf$constant('Hello, Tensorflow!')
sess$run(hello)
library(tensorflow)
install_tensorflow(version = "gpu")
install_keras()
setwd("~/R/stat_modeling/project/BlogFeedback")
bt <- read.csv("blogData_train.csv",header = F)
bt
bt[1,]
bt[[1,] = 0]
bt[bt[1,] = 0]
bt[1,]
post_test = read.csv('labeled_data/post_test.csv')
post_test
post_test = read.csv('labeled_data/post_test.csv')
post_test = read.csv('labeled_data\post_test.csv')
post_test = read.csv('labeled_data//post_test.csv')
post_test = read.csv('labeled_data//post_data.csv')
post_test = read.csv('labeled_data\post_data.csv')
post_test = read.csv('labeled_data/post_data.csv')
post_test = read.csv('dataframes/post_data.csv')
post_test = read.csv('dataframes//post_data.csv')
post_test = read.csv('post_data.csv')
wd
getwd
getwd()
setwd("~/dataframes")
post_test = read.csv('post_data.csv')
getwd()
post_test
getwd()
setwd("~/GitHub/ARL/data_processing")
post_test
setwd("~/dataframes")
post_test = read.csv('post_data.csv')
post_test = read.csv('/dataframes/post_data.csv')
post_test = read.csv('dataframes/post_data.csv')
post_test
#setwd('C:\\Users\\vaibhav\\Documents\\UVA\\Fall\\Capstone\\Code\\ARL\\data_processing')
post.data <- read.csv('dataframes\\post_data.csv')
post.rating.data <- data.frame()
post.test.data <- data.frame()
vector.is.numeric <- function(x) {
#Function that gives true if there are only numeric values
return(!anyNA(as.numeric(as.character(x))))
}
for (i in 1: 18) {
if (i %% 2 == 1) {
post.rating.data <- rbind(post.rating.data, as.data.frame(post.data[i,]))
} else {
post.test.data <- rbind(post.test.data, as.data.frame(post.data[i,]))
}
}
#Finding which columns are numeric
nums <- unlist(lapply(post.test.data, vector.is.numeric))
nums <- names(which(nums == TRUE))
#Removing all the numeric columns from the test data
`%ni%` <- Negate(`%in%`)
post.clean.test.data <- subset(post.test.data, select = names(post.test.data) %ni% nums)
#Removing all the columns from the rating data that were removed from the test data
post.clean.rating.data <- subset(post.rating.data, select = names(post.rating.data) %ni% nums)
post.clean.rating.data[1,]
post.clean.test.data[1,]
post.clean.test.data
View(post.clean.test.data)
View(post.clean.test.data)
