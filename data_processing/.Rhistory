}
#Part A.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(1.2, -1.8, 0.2, -1.6, -1.1, -0.9, 0.0, -1.6, -0.3, 0.9, 1.2, 0.2, 0.8, -0.2, 0.3, -0.6, 1.0, 0.8, 1.4, 0.0)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = 5)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
#Part B.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
beta.matrix
#[1] 5.252329
solve((t(X) * x))
(t(X) * x)
(t(X) %*% x)
X
X
t(X)
#[1] 5.252329
solve((t(X) %*% X))
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(0.8, -1.2, 1.5, 0.9, 1.8, 1.8, 0.0, -1.6, -0.4, -0.5, -0.6, -0.9, -0.5, -0.5, 0.9, -0.3, 0.1, -1.1, 0.7, -0.7)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = 5)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
#[1] 5.252329
solve((t(X) %*% X))
#[1] 5.252329
solve((t(X) %*% X))*var(y)
#Part A.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
#Variance of beta2
var(b2.vals)
#For part C
solve((t(X) %*% X)) * var(y)
var(y)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(1.2, -1.8, 0.2, -1.6, -1.1, -0.9, 0.0, -1.6, -0.3, 0.9, 1.2, 0.2, 0.8, -0.2, 0.3, -0.6, 1.0, 0.8, 1.4, 0.0)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
b1.vals <- numeric(n.rep)
b0.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = 5)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
#Part B.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
#For part C
solve((t(X) %*% X)) * var(y)
var(x2)
var(x1)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(0.8, -1.2, 1.5, 0.9, 1.8, 1.8, 0.0, -1.6, -0.4, -0.5, -0.6, -0.9, -0.5, -0.5, 0.9, -0.3, 0.1, -1.1, 0.7, -0.7)
var(x1)
var(x2)
b0.vals
1.111838*2
var(b0.vals)
var(b0.vals)*2
(t(X) %*% X)
(t(X) %*% X)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(0.8, -1.2, 1.5, 0.9, 1.8, 1.8, 0.0, -1.6, -0.4, -0.5, -0.6, -0.9, -0.5, -0.5, 0.9, -0.3, 0.1, -1.1, 0.7, -0.7)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(0.8, -1.2, 1.5, 0.9, 1.8, 1.8, 0.0, -1.6, -0.4, -0.5, -0.6, -0.9, -0.5, -0.5, 0.9, -0.3, 0.1, -1.1, 0.7, -0.7)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = 5)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
solve((t(X) %*% X))
solve((t(X) %*% X))*5
solve((t(X) %*% X))*5*5
#Part A.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
#For part C
solve((t(X) %*% X)) * sd ^ 2
#For part C
solve((t(X) %*% X)) * (sd ^ 2)
sd ^ 2
sd <- 5
#For part C
solve((t(X) %*% X)) * (sd ^ 2)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(1.2, -1.8, 0.2, -1.6, -1.1, -0.9, 0.0, -1.6, -0.3, 0.9, 1.2, 0.2, 0.8, -0.2, 0.3, -0.6, 1.0, 0.8, 1.4, 0.0)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
sd <- 5
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = sd)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
#Part B.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
#For part C
solve((t(X) %*% X)) * sd ^ 2
alpha/4
1 - alpha/4
f.crit
f.scheffe.crit
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(0.8, -1.2, 1.5, 0.9, 1.8, 1.8, 0.0, -1.6, -0.4, -0.5, -0.6, -0.9, -0.5, -0.5, 0.9, -0.3, 0.1, -1.1, 0.7, -0.7)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
sd <- sqrt(5)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = sd)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
#Part A.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
#For part C
solve((t(X) %*% X)) * (sd ^ 2)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(1.2, -1.8, 0.2, -1.6, -1.1, -0.9, 0.0, -1.6, -0.3, 0.9, 1.2, 0.2, 0.8, -0.2, 0.3, -0.6, 1.0, 0.8, 1.4, 0.0)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
sd <- 5
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = sd)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
#Part B.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
sd <- sqrt(5)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(1.2, -1.8, 0.2, -1.6, -1.1, -0.9, 0.0, -1.6, -0.3, 0.9, 1.2, 0.2, 0.8, -0.2, 0.3, -0.6, 1.0, 0.8, 1.4, 0.0)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
sd <- sqrt(5)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
rm(list=ls())
x1
x1 <- c(0.5, -2.2, -0.1, -1.4, -1.0, -0.4, 0.9, -1.5, -0.3, 0.4, 0.6, 0.4, 1.1, 0.0, 0.2, -0.3, -0.2, 1.5, 1.8, 0.1)
x2 <- c(1.2, -1.8, 0.2, -1.6, -1.1, -0.9, 0.0, -1.6, -0.3, 0.9, 1.2, 0.2, 0.8, -0.2, 0.3, -0.6, 1.0, 0.8, 1.4, 0.0)
n <- length(x1)
n.rep <- 500
ones.vector <- rep(1, times = n)
X <- matrix(c(ones.vector, x1, x2), ncol = 3, nrow = n)
sd <- sqrt(5)
b0.vals <- numeric(n.rep)
b1.vals <- numeric(n.rep)
b2.vals <- numeric(n.rep)
for (i in 1:n.rep) {
y <- 25 + 2 * x1 - 3 * x2 + rnorm(n, mean = 0, sd = sd)
Y <- matrix(y, ncol = 1)
beta.matrix <- solve(t(X) %*% X) %*% t(X) %*% Y
b0.vals[i] <- beta.matrix[1]
b1.vals[i] <- beta.matrix[2]
b2.vals[i] <- beta.matrix[3]
}
#Part B.
#Variance of beta0
var(b0.vals)
#Variance of beta1
var(b1.vals)
#Variance of beta2
var(b2.vals)
#For part C
solve((t(X) %*% X)) * sd ^ 2
x <- seq(from=1, to=10, by=0.5)
n <- length(x)
x.bar <- sum(x) / n
sxx <- sum((x - x.bar) ^ 2)
beta0 <- 50
beta1 <- 10
sig2 <- 16
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
alpha <- 0.05
#Number of simulations is stored in n.rep
n.rep <- 500
#Vector that stores the result weather beta0 is contained in the confidence interaval
beta0.contains <- logical(n.rep)
#Vector that stores the result weather beta1 is contained in the confidence interaval
beta1.contains <- logical(n.rep)
#Vector that stores the result weather beta0 is contained in the bonferroni confidence interaval
beta0.bonf.contains <- logical(n.rep)
#Vector that stores the result weather beta1 is contained in the bonferroni confidence interaval
beta1.bonf.contains <- logical(n.rep)
#Vector that stores the result weather beta0 is contained in the sheffe's s confidence interaval
beta0.scheffe.contains <- logical(n.rep)
#Vector that stores the result weather beta1 is contained in the sheffe's s confidence interaval
beta1.scheffe.contains <- logical(n.rep)
#Vector that stores the result weather the f- statistic is less than the f critical value for the joint confidence interval calculations
f.statistic.satisfied <- logical(n.rep)
#p = (regressors + 1)
p <- 2
#T- critical value
t.crit <- qt(alpha / 2, df = n-1, lower.tail = FALSE)
#T- critical value for bonferroni method
t.bonf.crit <- qt(alpha / (2*p), df = n - p, lower.tail = FALSE)
#F- critical value
f.crit <- qf(alpha, df1 = p, df2 = n - p, lower.tail = FALSE)
#F- critical value for scheffe's method
f.scheffe.crit <- sqrt(2 * qf(alpha, df1 = p, df2 = n - p, lower.tail = FALSE))
for (i in 1:n.rep) {
y <- beta0 + beta1 * x + rnorm(n, mean = 0, sd = sqrt(sig2))
y.bar <- sum(y)/ n
fit <- lm(y ~ x)
beta0.hat <- coefficients(fit)[[1]]
beta1.hat <- coefficients(fit)[[2]]
sxy <- sum(y * (x - x.bar))
ss.t <- sum((y - y.bar) ^ 2)
ss.r <- beta1.hat * sxy
ss.res <- ss.t - ss.r
MS.R <- ss.r
MS.Res <- ss.res / (n - 2)
#Part A calculation
se.beta1 <- sqrt(MS.Res / sxx)
se.beta0 <- sqrt(MS.Res * (1/n + x.bar ^ 2 / sxx))
beta1.contains[i] <- (abs(beta1.hat - beta1) <= t.crit * se.beta1) #CI on slope
beta0.contains[i] <- (abs(beta0.hat - beta0) <= t.crit * se.beta0) #CI on intercept
#Part B calculation
beta0.diff <- (beta0.hat - beta0)
beta1.diff <- (beta1.hat - beta1)
joint.confidence.region <- (n * beta0.diff ^ 2 + 2 * sum(x * beta1.diff * beta0.diff) + sum(x ^ 2 * beta1.diff ^ 2)) / (p * MS.Res)
f.statistic.satisfied[i] <- (joint.confidence.region <= f.crit)
#Part C calculation
beta1.bonf.contains[i] <- (abs(beta1.hat - beta1) <= t.bonf.crit * se.beta1) #CI on slope
beta0.bonf.contains[i] <- (abs(beta0.hat - beta0) <= t.bonf.crit * se.beta0) #CI on intercept
#Part D calculation
beta1.scheffe.contains[i] <- (abs(beta1.hat - beta1) <= f.scheffe.crit * se.beta1) #CI on slope
beta0.scheffe.contains[i] <- (abs(beta0.hat - beta0) <= f.scheffe.crit * se.beta0) #CI on intercept
}
#Answer Part A
length(beta1.contains[beta1.contains == TRUE]) / n.rep
#[1] 0.946
length(beta0.contains[beta0.contains == TRUE]) / n.rep
#Answer Part B
length(f.statistic.satisfied[f.statistic.satisfied == TRUE]) / n.rep
#Answer Part C
length(beta1.bonf.contains[beta1.bonf.contains == TRUE]) / n.rep
#[1] 0.98
length(beta0.bonf.contains[beta0.bonf.contains == TRUE]) / n.rep
#Answer Part D
length(beta1.scheffe.contains[beta1.scheffe.contains == TRUE]) / n.rep
#[1] 0.988
length(beta0.scheffe.contains[beta0.scheffe.contains == TRUE]) / n.rep
9997/7
9996/7
9994/7
9993/7
9991/7
9989/7
9989-7
9982/7
library(tm)
library(SnowballC)
library(wordcloud)
library(tidyverse)
library(tidytext)
post.data <- read.csv('dataframes\\post_data.csv')
post.rating.data <- data.frame()
post.test.data <- data.frame()
vector.is.numeric <- function(x) {
#Function that gives true if there are only numeric values
return(!anyNA(as.numeric(as.character(x))))
}
for (i in 1: 18) {
if (i %% 2 == 1) {
post.rating.data <- rbind(post.rating.data, as.data.frame(post.data[i,]))
} else {
post.test.data <- rbind(post.test.data, as.data.frame(post.data[i,]))
}
}
#Finding which columns are numeric
nums <- unlist(lapply(post.test.data, vector.is.numeric))
nums <- names(which(nums == TRUE))
#Removing all the numeric columns from the test data
`%ni%` <- Negate(`%in%`)
post.clean.test.data <- subset(post.test.data, select = names(post.test.data) %ni% nums)
#Removing all the columns from the rating data that were removed from the test data
post.clean.rating.data <- subset(post.rating.data, select = names(post.rating.data) %ni% nums)
#Getting overall score dataframe of each author
overall.score <- subset(post.data[19,], select = names(post.data) %ni% nums)
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(post.clean.test.data))
rating.entries <- as.vector(unlist(post.clean.rating.data))
#Extracting all the numerical entries from the test data
numerical.indices <- c()
for (i in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[i]) == TRUE) {
numerical.indices <- c(numerical.indices, i)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
master.df <- data.frame(matrix(ncol = 4, nrow = 0))
x <- c("document", "term", "count", "rating")
colnames(master.df) <- x
for (i in 1 : length(test.entries)) {
test.corpus <- Corpus(VectorSource(test.entries[i]))
"
Next we normalize the texts in the reviews using a series of pre-processing steps:
1. Switch to lower case
2. Remove punctuation marks
3. Remove extra whitespaces
4. Remove stop words
5. Stemmatize the words
"
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
new.entry <- cbind(data.frame(tidy(test.corpus.dtm)), data.frame('rating' = rating.entries[i]))
master.df <- rbind(master.df, new.entry)
}
View(master.df)
setwd('C:\\Users\\vaibhav\\Documents\\UVA\\Fall\\Capstone\\Code\\ARL\\data_processing')
library(tm)
library(SnowballC)
library(wordcloud)
library(tidyverse)
library(tidytext)
post.data <- read.csv('dataframes\\post_data.csv')
post.rating.data <- data.frame()
post.test.data <- data.frame()
vector.is.numeric <- function(x) {
#Function that gives true if there are only numeric values
return(!anyNA(as.numeric(as.character(x))))
}
for (i in 1: 18) {
if (i %% 2 == 1) {
post.rating.data <- rbind(post.rating.data, as.data.frame(post.data[i,]))
} else {
post.test.data <- rbind(post.test.data, as.data.frame(post.data[i,]))
}
}
#Finding which columns are numeric
nums <- unlist(lapply(post.test.data, vector.is.numeric))
nums <- names(which(nums == TRUE))
#Removing all the numeric columns from the test data
`%ni%` <- Negate(`%in%`)
post.clean.test.data <- subset(post.test.data, select = names(post.test.data) %ni% nums)
#Removing all the columns from the rating data that were removed from the test data
post.clean.rating.data <- subset(post.rating.data, select = names(post.rating.data) %ni% nums)
#Getting overall score dataframe of each author
overall.score <- subset(post.data[19,], select = names(post.data) %ni% nums)
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(post.clean.test.data))
rating.entries <- as.vector(unlist(post.clean.rating.data))
#Extracting all the numerical entries from the test data
numerical.indices <- c()
for (i in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[i]) == TRUE) {
numerical.indices <- c(numerical.indices, i)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
master.df <- data.frame(matrix(ncol = 4, nrow = 0))
x <- c("document", "term", "count", "rating")
colnames(master.df) <- x
for (i in 1 : length(test.entries)) {
test.corpus <- Corpus(VectorSource(test.entries[i]))
"
Next we normalize the texts in the reviews using a series of pre-processing steps:
1. Switch to lower case
2. Remove punctuation marks
3. Remove extra whitespaces
4. Remove stop words
5. Stemmatize the words
"
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
new.entry <- cbind(data.frame(tidy(test.corpus.dtm)), data.frame('rating' = rating.entries[i]))
master.df <- rbind(master.df, new.entry)
}
View(master.df)
master.df %>% group_by(term) %>% summarise(sum = sum(count))
x <- master.df %>% group_by(term) %>% summarise(sum = sum(count))
arrange(x, desc(sum()))
arrange(x, desc(sum)
)
