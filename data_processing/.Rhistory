test.corpus.dtm <- DocumentTermMatrix(test.corpus)
test.corpus.dtm
inspect(review_dtm[1:5, 1:5])
inspect(test.corpus[1:5, 1:5])
inspect(test.corpus.dtm[1:5, 1:5])
findFreqTerms(test.corpus.dtm, 1000)
?findFreqTerms
findFreqTerms(test.corpus.dtm)
findFreqTerms(test.corpus.dtm, 50)
findFreqTerms(test.corpus.dtm, lowfreq = 50)
?findFreqTerms
findFreqTerms(test.corpus.dtm, lowfreq = 1, highfreq = 5)
findFreqTerms(test.corpus.dtm)
findFreqTerms(test.corpus.dtm, lowfreq = 1, highfreq = 5)
findFreqTerms(test.corpus.dtm, lowfreq = 1, highfreq = 3)
findFreqTerms(test.corpus.dtm, lowfreq = 1, highfreq = 2)
findFreqTerms(test.corpus.dtm, lowfreq = 1, highfreq = 3)
findFreqTerms(test.corpus.dtm, lowfreq = 1, highfreq = 2)
findFreqTerms(test.corpus.dtm, lowfreq = 1, highfreq = 1)
findFreqTerms(test.corpus.dtm, lowfreq = 0, highfreq = 1)
test <- 'love loverr loves love\\'s lovingly'
test <- 'love loverr loves love\'s lovingly'
test
library(SnowballC)
install.packages('SnowballC')
install.packages('wordcloud')
library(SnowballC)
library(wordcloud)
text_tokens(text, stemmer = 'en')
SnowballC::wordStem(text, stemmer = 'en')
SnowballC::wordStem(text)
SnowballC::wordStem(c(text))
?wordStem
wordStem(c(text), language = 'en')
wordStem(c("win", "winning", "winner"))
text
wordStem(c(test), language = 'en')
wordStem(c(test))
test
wordStem(c(split(test, ' ')))
split(test, ' ')
unlist(split(test, ' '))
unlist(strsplit(test, ' '))
strsplit(test, ' ')
unlist(strsplit(test, ' '))
wordStem(unlist(strsplit(test, ' ')))
test
wordStem(unlist(test))
wordStem(c("win", "winning", "winner"))
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
review_corpus <- tm_map(review_corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
test.corpus.dtm
#Inspecting the first 5 documents and the first 5 words in the corpus
inspect(test.corpus.dtm[1:5, 1:5])
findMostFreqTerms(test.corpus.dtm)
findFreqTerms(test.corpus.dtm, highfreq = 1)
findFreqTerms(test.corpus.dtm, highfreq = 1, lowfreq = 1)
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
findFreqTerms(test.corpus.dtm, highfreq = 1, lowfreq = 1)
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
test.corpus.dtm
#Inspecting the first 5 documents and the first 5 words in the corpus
inspect(test.corpus.dtm[1:5, 1:5])
findMostFreqTerms(test.corpus)
findMostFreqTerms(test.corpus.dtm)
findMostFreqTerms(test.corpus.dtm)
findFreqTerms(test.corpus.dtm, 50)
most.freq.terms <- findFreqTerms(test.corpus.dtm, 50)
freq <- data.frame(sort(colSums(as.matrix(test.corpus.dtm)), decreasing=TRUE))
freq
freq[1:5,]
head(freq, 5)
freq.df <- head(freq, 5)
findFreqTerms(review_dtm, 1000)[1:5]
findFreqTerms(test.corpus.dtm, 1000)[1:5]
findFreqTerms(test.corpus.dtm, 1000)
findFreqTerms(test.corpus.dtm, 50)
findFreqTerms(test.corpus.dtm, 150)
findFreqTerms(test.corpus.dtm, 10)
freq.df <- head(freq, 5)
freq.df$sort.colSums.as.matrix.test.corpus.dtm....decreasing...TRUE.
freq.df
`colnames<-`(freq.df, c('word', 'freq'))
dim(freq.df)
rownames(freq.df)
colnames(freq.df) <- c('frequency')
freq.df
barplot(y = freq.df$frequency, x = rownames(freq.df))
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words")
freq.df
barplot(y = freq.df$frequency, x = rownames(freq.df), ylim = (80, 160))
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words", ylim = c(80, 160))
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words")
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words", ylim = c(0, 150))
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words", ylim = c(0, 160))
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words", ylim = c(0, 150))
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words", ylim = c(0, 160))
barplot(freq.df$frequency, names.arg=rownames(freq.df), col=c("beige","orange"), ylab="Count of words", ylim = c(0, 160), main = 'Most frequent words')
library(XML)
install.packages('XML')
library(XML)
library(tm)
# read some news data from an XML file and transform it into a corpus. the following
# data frame will have three columns:  id (document identifier), t (title), date (rough date)
# and c (content).
document.data.frame = xmlToDataFrame("Data/news_documents.xml", stringsAsFactors = FALSE)
# read some news data from an XML file and transform it into a corpus. the following
# data frame will have three columns:  id (document identifier), t (title), date (rough date)
# and c (content).
document.data.frame = xmlToDataFrame("news_documents.xml", stringsAsFactors = FALSE)
# read some news data from an XML file and transform it into a corpus. the following
# data frame will have three columns:  id (document identifier), t (title), date (rough date)
# and c (content).
document.data.frame = xmlInternalTreeParse("news_documents.xml", stringsAsFactors = FALSE)
# read some news data from an XML file and transform it into a corpus. the following
# data frame will have three columns:  id (document identifier), t (title), date (rough date)
# and c (content).
document.data.frame = xmlInternalTreeParse("news_documents.xml")
# read some news data from an XML file and transform it into a corpus. the following
# data frame will have three columns:  id (document identifier), t (title), date (rough date)
# and c (content).
document.data.frame = xmlToDataFrame("news_documents.xml", stringsAsFactors = FALSE)
# read some news data from an XML file and transform it into a corpus. the following
# data frame will have three columns:  id (document identifier), t (title), date (rough date)
# and c (content).
document.data.frame = xmlToDataFrame("news_documents.xml", stringsAsFactors = FALSE)
setwd('C:\\Users\\vaibhav\\Documents\\UVA\\Fall\\Capstone\\Code\\ARL\\data_processing')
# setwd('C:\\Users\\vaibhav\\Documents\\UVA\\Fall\\Capstone\\Code\\ARL\\data_processing')
library(tm)
library(SnowballC)
library(wordcloud)
post.data <- read.csv('dataframes\\post_data.csv')
post.rating.data <- data.frame()
post.test.data <- data.frame()
View(post.data)
# setwd('C:\\Users\\vaibhav\\Documents\\UVA\\Fall\\Capstone\\Code\\ARL\\data_processing')
library(tm)
library(SnowballC)
library(wordcloud)
post.data <- read.csv('dataframes\\post_data.csv')
post.rating.data <- data.frame()
post.test.data <- data.frame()
vector.is.numeric <- function(x) {
#Function that gives true if there are only numeric values
return(!anyNA(as.numeric(as.character(x))))
}
for (i in 1: 18) {
if (i %% 2 == 1) {
post.rating.data <- rbind(post.rating.data, as.data.frame(post.data[i,]))
} else {
post.test.data <- rbind(post.test.data, as.data.frame(post.data[i,]))
}
}
post.rating.data
View(post.rating.data)
View(post.test.data)
#Finding which columns are numeric
nums <- unlist(lapply(post.test.data, vector.is.numeric))
nums <- names(which(nums == TRUE))
#Removing all the numeric columns from the test data
`%ni%` <- Negate(`%in%`)
post.clean.test.data <- subset(post.test.data, select = names(post.test.data) %ni% nums)
#Removing all the columns from the rating data that were removed from the test data
post.clean.rating.data <- subset(post.rating.data, select = names(post.rating.data) %ni% nums)
View(post.clean.test.data)
View(post.clean.rating.data)
nrow(post.clean.rating.data)
nrow(post.clean.test.data)
vector.is.numeric('c')
isNA(as.numeric(as.character('x')))
is.na(as.numeric(as.character('x')))
i <- 1
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(post.clean.test.data[,i]))
rating.entries <- as.vector(unlist(post.clean.rating.data[,i]))
test.entries
numerical.indices <- c()
for (j in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[j]) == TRUE) {
numerical.indices <- c(numerical.indices, j)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
test.entries
rating.entries
post.data[19,]
subset(post.data[19,], select = names(post.data) %ni% nums)
#Getting overall score of each author
overall.score <- subset(post.data[19,], select = names(post.data) %ni% nums)
master.data <- data.frame(row.names = c('author', 'document.term.matrix', 'corpus', 'overall.rating'))
i <- 1
#Starting author wise analysis
for (i in 1:ncol(post.clean.test.data)){
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(post.clean.test.data[,i]))
rating.entries <- as.vector(unlist(post.clean.rating.data[,i]))
numerical.indices <- c()
for (j in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[j]) == TRUE) {
numerical.indices <- c(numerical.indices, j)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
"
Next we normalize the texts in the reviews using a series of pre-processing steps:
1. Switch to lower case
2. Remove punctuation marks
3. Remove extra whitespaces
4. Remove stop words
5. Stemmatize the words
"
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
}
master.data
master.data <- data.frame(col.names = c('author', 'document.term.matrix', 'corpus', 'overall.rating'))
View(master.data)
?data.frame
master.data <-setNames(data.frame(matrix(ncol = 4, nrow = 0)), c('author', 'document.term.matrix', 'corpus', 'overall.rating'))
View(master.data)
master.data <-setNames(data.frame(matrix(ncol = ncol(post.clean.test.data), nrow = 0)), names(post.clean.test.data))
View(master.data)
master.data[[names(post.clean.test.data[1])]] <- c(1,23)
master.data <-setNames(data.frame(matrix(ncol = ncol(post.clean.test.data), nrow = 3)), names(post.clean.test.data))
master.data[[names(post.clean.test.data[1])]] <- c(1,23)
master.data[[names(post.clean.test.data[1])]] <- c(1,23, 34)
View(master.data)
master.data <-setNames(data.frame(matrix(ncol = ncol(post.clean.test.data), nrow = 3)), names(post.clean.test.data))
#Starting author wise analysis
for (i in 1:ncol(post.clean.test.data)){
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(post.clean.test.data[,i]))
rating.entries <- as.vector(unlist(post.clean.rating.data[,i]))
numerical.indices <- c()
for (j in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[j]) == TRUE) {
numerical.indices <- c(numerical.indices, j)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
"
Next we normalize the texts in the reviews using a series of pre-processing steps:
1. Switch to lower case
2. Remove punctuation marks
3. Remove extra whitespaces
4. Remove stop words
5. Stemmatize the words
"
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
output.column <- c(test.corpus, test.corpus.dtm, overall.score[i])
master.data[[names(post.clean.test.data[i])]] <- output.column
}
View(master.data)
ncol(post.clean.test.data)
master.data <- setNames(data.frame(matrix(ncol = ncol(post.clean.test.data), nrow = 3)), names(post.clean.test.data))
#Starting author wise analysis
for (i in 1:ncol(post.clean.test.data)){
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(post.clean.test.data[,i]))
rating.entries <- as.vector(unlist(post.clean.rating.data[,i]))
numerical.indices <- c()
for (j in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[j]) == TRUE) {
numerical.indices <- c(numerical.indices, j)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
"
Next we normalize the texts in the reviews using a series of pre-processing steps:
1. Switch to lower case
2. Remove punctuation marks
3. Remove extra whitespaces
4. Remove stop words
5. Stemmatize the words
"
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
output.column <- c(test.corpus, test.corpus.dtm, overall.score[i])
master.data[[names(post.clean.test.data)[i]]] <- output.column
}
names(post.clean.test.data)[i]
i
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(post.clean.test.data[,i]))
rating.entries <- as.vector(unlist(post.clean.rating.data[,i]))
numerical.indices <- c()
for (j in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[j]) == TRUE) {
numerical.indices <- c(numerical.indices, j)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
"
Next we normalize the texts in the reviews using a series of pre-processing steps:
1. Switch to lower case
2. Remove punctuation marks
3. Remove extra whitespaces
4. Remove stop words
5. Stemmatize the words
"
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
output.column <- c(test.corpus, test.corpus.dtm, overall.score[i])
output.column
test.corpus.dtm
overall.score[i]
test.corpus.dtm
test.corpus
str(test.corpus)
data.frame(test.corpus, test.corpus.dtm, as.numeric(overall.score[i]))
c('sd', 2)
data.frame(test.corpus)
rbind(test.corpus, test.corpus.dtm)
rbind(as.matrix(test.corpus), as.matrix(test.corpus.dtm))
rbind(as.matrix(test.corpus))
as.data.frame(as.matrix(test.corpus))
which(overall.score >= threshold)
overall.score >= threshold
threshold <- 12
which(overall.score >= threshold)
overall.score >= threshold
overall.score
overall.score[1]
as.vector(overall.score)
as.vector(overall.score[[]])
as.vector(overall.score[)
as.vector(overall.score[])
which(overall.score >= threshold)
overall.score >= threshold
as.vector(overall.score) >= threshold
overall.score[19]
overall.score[19,]
overall.score[1,]
overall.score[19,]
overall.score[1,]
as.vector(overall.score)
as.vector(overall.score[1,])
which(overall.score[1,] >= threshold)
which(as.vector(overall.score[1,]) >= threshold)
threshold
names(post.clean.test.data)[i]
overall.score['x2564']
overall.score['X2564']
overall.score['X2564'][1]
overall.score['X2564'][19]
overall.score['X2564'][1]
overall.score['X2564'][[1]]
as.vector(overall.score['X2564'][[1]])
as.vector(overall.score[[1]])
as.vector(overall.score[[2]])
as.vector(overall.score[[]])
as.vector(overall.score[[4]])
overall.score
as.numeric(overall.score[[4]])
as.numeric(overall.score)
#Overall score vector
overall.score.vector <- as.numeric(overall.score)
which(overall.score.vector >= 12)
overall.score.vector
overall.score[which(overall.score[1,] > 9),]
overall.score[which(overall.score[1,] > 10),]
for (i in 1:ncol(overall.score)){
print(as.vector(overall.score[[i]]))
}
for (i in 1:ncol(overall.score)){
print(as.numeric(as.vector(overall.score[[i]]))))
}
for (i in 1:ncol(overall.score)){
print(as.numeric(as.vector(overall.score[[i]])))
}
as.numeric(as.vector(overall.score[[]]))
as.numeric(as.vector(overall.score[[2]]))
as.numeric(as.vector(overall.score[[1]]))
as.numeric(as.vector(overall.score[[19]]))
threshold <- 10
higher.threshold <- c()
for (i in 1:ncol(overall.score)){
if (as.numeric(as.vector(overall.score[[i]])) >= threshold)
higher.threshold <- c(higher.threshold, i)
}
higher.threshold
names(post.clean.test.data)[higher.threshold]
names(post.clean.test.data)[-higher.threshold]
#Authors with greater than or equal to threshold scores
high.scoring.authors <- names(post.clean.test.data)[higher.threshold]
#Authors with less than threshold scores
high.scoring.authors <- names(post.clean.test.data)[-higher.threshold]
#Authors with greater than or equal to threshold scores
high.scoring.authors <- names(post.clean.test.data)[higher.threshold]
#Authors with less than threshold scores
low.scoring.authors <- names(post.clean.test.data)[-higher.threshold]
high.clean.test.data <- subset(post.clean.test.data, select = names(post.clean.test.data) %in% high.scoring.authors)
View(high.clean.test.data)
#Authors with less than threshold scores
low.clean.test.data <- subset(post.clean.test.data, select = names(post.clean.test.data) %ni% high.scoring.authors)
View(low.clean.test.data)
Get.Document.Term.Matrix <- function(x) {
#Extracting all the entries from the data frame
test.entries <- as.vector(unlist(x))
#Extracting all the numerical entries from the test data
numerical.indices <- c()
for (i in 1:length(test.entries)) {
if (vector.is.numeric(test.entries[i]) == TRUE) {
numerical.indices <- c(numerical.indices, i)
}
}
#Removing the numerical entries from the test data and their corresponding ratings
test.entries <- test.entries[-numerical.indices]
rating.entries <- rating.entries[-numerical.indices]
#Creating a corpus from the testing data
test.corpus <- Corpus(VectorSource(test.entries))
"
Next we normalize the texts in the reviews using a series of pre-processing steps:
1. Switch to lower case
2. Remove punctuation marks
3. Remove extra whitespaces
4. Remove stop words
5. Stemmatize the words
"
test.corpus <- tm_map(test.corpus, content_transformer(tolower))
test.corpus <- tm_map(test.corpus, removePunctuation)
test.corpus <- tm_map(test.corpus, stripWhitespace)
test.corpus <- tm_map(test.corpus, removeWords, c("the", "and", stopwords("english")))
test.corpus <- tm_map(test.corpus, stemDocument, language = "english")
"
To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
"
test.corpus.dtm <- DocumentTermMatrix(test.corpus)
return (test.corpus.dtm)
}
high.dtm <- Get.Document.Term.Matrix(high.clean.test.data)
low.dtm <- Get.Document.Term.Matrix(low.clean.test.data)
high.dtm
low.dtm
overall.score
as.vector(unlist(overall.score))
?unlist
low.dtm
low.dtm[1]
low.dtm[[1]]
inspect(low.dtm[1:5, 1:5])
