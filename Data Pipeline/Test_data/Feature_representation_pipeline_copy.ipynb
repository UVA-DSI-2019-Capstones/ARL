{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing required dependencies ['numpy']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0561c626e827>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     raise ImportError(\n\u001b[1;32m---> 19\u001b[1;33m         \"Missing required dependencies {0}\".format(missing_dependencies))\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdependency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing required dependencies ['numpy']"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_distributor_init' from 'numpy' (C:\\Users\\marti\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-10d59dea6fa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\parsing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001b[0m\u001b[0;32m      5\u001b[0m                             \u001b[0mstrip_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrip_short\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrip_numeric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                             \u001b[0mstrip_non_alphanum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\parsing\\preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;31m# Allow distributors to run custom init code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_distributor_init' from 'numpy' (C:\\Users\\marti\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "import re\n",
    "import string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1fd3f046b558>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# find_files is given a directory. It creates a database containing the name and relative path for each file\n",
    "\n",
    "def find_files(subfolder_path):\n",
    "    #This code creates a database with every file and its path\n",
    "    dir = Path(os.getcwd())\n",
    "    #Add directory where your files are:\n",
    "    newdir = dir / subfolder_path\n",
    "\n",
    "\n",
    "    #subfolders = os.listdir(newdir)\n",
    "    subfolders = ['Train_data', 'Test_data']\n",
    "    dense_list = [os.listdir(newdir / subfolder) for subfolder in subfolders]\n",
    "    paired_list = zip(dense_list, subfolders)\n",
    "\n",
    "    audio_files = [(item, label) for sublist, label in paired_list for item in sublist]\n",
    "    audio_file_list, path_list = zip(*audio_files)\n",
    "    columns = ['file', 'relative_path']\n",
    "\n",
    "    common_prefix = os.path.commonprefix([dir, newdir])\n",
    "    relative_path = os.path.relpath(newdir, common_prefix)\n",
    "    print(relative_path)\n",
    "\n",
    "    #Using os.join.path and Path() leads to a Windows path, so I had to do it this way\n",
    "    relative_path = [relative_path + '/' + path for path in path_list]\n",
    "    print(relative_path)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    print(audio_file_list)\n",
    "    print(relative_path)\n",
    "    df.file = audio_file_list\n",
    "    df.relative_path = relative_path\n",
    "    return df\n",
    "\n",
    "section_number = 1\n",
    "section_string = '_' + str(section_number) + '.'\n",
    "\n",
    "df = find_files('Feature_representations')\n",
    "\n",
    "non_ind = list(map(lambda x: 'test' in x or 'train' in x, list(df['file'].apply(lambda x: x.lower()))))\n",
    "df = df.drop([i for i, x in enumerate(non_ind) if not x])\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "test_ind = list(map(lambda x: 'test' in x, list(df['file'].apply(lambda x: x.lower()))))\n",
    "df['test'] = df['file'][[i for i, x in enumerate(test_ind) if x]]\n",
    "df['train'] = df['file'][[i for i, x in enumerate(test_ind) if not x]]\n",
    "ind_list = df['file'].apply(lambda x: section_string in x).tolist()\n",
    "df.drop([i for i, x in enumerate(ind_list) if not x], inplace =True)\n",
    "\n",
    "\n",
    "# loadGloveData takes the dimension of the Glove word vector as input. In creates a numpy version of the word vectors that is saved to the disk. \n",
    "\n",
    "def loadGloveData(cur_dim):\n",
    "\n",
    "    #%% Set path to Glove word vector folder\n",
    "    dir = Path(os.getcwd())\n",
    "    wvpack = \"glove.6B.\"+str(cur_dim)+\"d.txt\"\n",
    "    file_1 = dir / \"glove.6B\" / wvpack\n",
    "\n",
    "    df = pd.read_csv(file_1, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    WV = {key: val.values for key, val in df.T.items()}\n",
    "    file_2 = os.path.join(dir,'glove_dic','wv_dic_{}.npy'.format(cur_dim))\n",
    "    np.save(file_2, WV) \n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "dim = [50, 100, 200, 300]\n",
    "dir = Path(os.getcwd())\n",
    "print(dir)\n",
    "\n",
    "for example in dim:\n",
    "    fname = os.path.join(dir,'glove_dic','wv_dic_{}.npy'.format(example))\n",
    "    if not os.path.isfile(fname):\n",
    "        print(example)\n",
    "        loadGloveData(example)\n",
    "\n",
    "# Create_train_test_w2v_matrices takes the dataframe containing the path to each train and test csv file, which is output by find_files. This function completes the processing required to create the word averaged representation of each piece of input data. The train and test sections coerce the input dataframe into the correct format, and the input is then feed to the docAveraging function. The final matrix representation is saved to the disk.\n",
    "\n",
    "def create_path(*args):\n",
    "    cur_path = os.getcwd()\n",
    "    for value in args:\n",
    "        cur_path  = os.path.join(cur_path, value)\n",
    "    return cur_path\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    def __init__(self, file, rel_path):\n",
    "        self.quest_num = int(re.search(r'\\d+', file).group())\n",
    "        if 'test' in file.lower():\n",
    "            self.mat_type = 'test'\n",
    "            #Test Set\n",
    "            test = pd.read_csv(create_path(rel_path, file))\n",
    "            column_names = test.iloc[0,:]\n",
    "            test.drop([0], inplace=True)\n",
    "            test.rename(columns = column_names, inplace=True)\n",
    "            label_col_bound = len([type(x) for x in test.columns.values if isinstance(x, str)])\n",
    "            test = test.iloc[:, :label_col_bound]\n",
    "            test['labels_1'] = test[['Label-1']].apply(lambda x: ''.join(x.astype(str)),axis=1)\n",
    "            test['labels_2'] = test[['Label-2']].apply(lambda x: ''.join(x.astype(str)),axis=1)\n",
    "            test = test[~test['labels_1'].str.contains(\"e\")]\n",
    "            test = test[~test['labels_2'].str.contains(\"e\")]\n",
    "            self.X = test['Dialogue'].apply(lambda x : x.lower().translate(Data.table))\n",
    "            self.Y = test['labels_2']\n",
    "        #Train set\n",
    "        elif 'train' in file.lower():\n",
    "            self.mat_type = 'train'\n",
    "            #Train set\n",
    "            train = pd.read_csv(create_path(rel_path, file))\n",
    "            train.drop([train.columns[0]], axis = 1, inplace=True)\n",
    "            train.dropna(inplace=True)\n",
    "            train['labels']=train.labels.apply(lambda x: ''.join([(3-len(str(x)))*'0',str(x)]))\n",
    "            self.X = train['Dialogue'].apply(lambda x : x.lower().translate(Data.table))\n",
    "            self.Y = train['labels']\n",
    "            \n",
    "def save_labels(Y, out_path):\n",
    "    if not os.path.isfile(out_path):\n",
    "        np.save(out_path, Y)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    #Map POS tag to first character lemmatize() accepts\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def Create_TFIDF_matrices(df, lemmatize = False):\n",
    "    lemtz = ''\n",
    "    if lemmatize == True:\n",
    "        lemtz = 'lemmatized'      \n",
    "    test_df = df[['test', 'relative_path']].dropna()\n",
    "    train_df = df[['train', 'relative_path']].dropna()\n",
    "    test_df.reset_index(inplace = True)\n",
    "    train_df.reset_index(inplace = True)\n",
    "    if len(test_df) <= len(train_df):\n",
    "        for i in range(min([len(test_df), len(train_df)])):  \n",
    "            train_data = Data(train_df.train[i], train_df.relative_path[i])\n",
    "# Attributes of train_data:   mat_type, quest_num, X_train, Y_train \n",
    "            #Sanity Check\n",
    "            if train_data.mat_type != 'train' or train_data.quest_num != str(i+1):\n",
    "                print('Expected matrix type train, received type {}'.format(train_data.mat_type))\n",
    "                print('Expected question # {}, received # {}'.format(i+1, train_data.quest_num))\n",
    "                print('error')\n",
    "            test_data = Data(test_df.test[i], test_df.relative_path[i])\n",
    "            #Sanity Check\n",
    "            if test_data.mat_type != 'test' or test_data.quest_num != str(i+1):\n",
    "                print('Expected matrix type test, received type {}'.format(test_data.mat_type))\n",
    "                print('Expected question # {}, received # {}'.format(i+1, test_data.quest_num))\n",
    "                print('error')\n",
    "            Y_train = train_data.Y\n",
    "            Y_test = test_data.Y\n",
    "            #Create TF-IDF Matrices (we want to tokenize the text before creating these)\n",
    "            if lemmatize==True: \n",
    "                lemmatizer =WordNetLemmatizer() \n",
    "                X_test = []\n",
    "                X_train = []\n",
    "                for sent in test_data.X:\n",
    "                    X_test.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])) \n",
    "                for sent in train_data.X:\n",
    "                    X_train.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])) \n",
    "            X_test = test_data.X\n",
    "            X_train = train_data.X\n",
    "#             #Flatten lists\n",
    "#             X_train = [item for sublist in X_train for item in sublist]\n",
    "#             X_test = [item for sublist in X_test for item in sublist]\n",
    "            print('Length of X_train: ' + str(len(X_train)))\n",
    "            print('Length of X_test: ' + str(len(X_test)))\n",
    "            for dim in range (100, 200, 100): #max 2100\n",
    "                tfidf_vectorizer = TfidfVectorizer(max_features = dim)\n",
    "                tfidf_matrix = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "                tfidf_matrix_Test = tfidf_vectorizer.transform(X_test).toarray()\n",
    "                file_4 = create_path('tfidf_matrices','TFIDF_train_{}Question{}_{}dim.npy'.format(lemtz, train_data.quest_num, dim))\n",
    "                file_5 = create_path('tfidf_matrices','TFIDF_test_{}Question{}_{}dim.npy'.format(lemtz, test_data.quest_num, dim))\n",
    "                np.save(file_4, tfidf_matrix)\n",
    "                np.save(file_5, tfidf_matrix_Test)\n",
    "            save_labels(Y_train, create_path('labels', '{}_labels_question{}'.format(train_data.mat_type, train_data.quest_num)))\n",
    "            save_labels(Y_test, create_path('labels', '{}_labels_question{}'.format(test_data.mat_type, test_data.quest_num)))\n",
    "\n",
    "\n",
    "def doc2Vec(corpus, corpus_test, dim):\n",
    "\n",
    "    trainingMatrix = np.zeros((0, dim))\n",
    "    testMatrix = np.zeros((0, dim))\n",
    "\n",
    "    doc_clean = [doc.split() for doc in corpus]\n",
    "    doc_Test = [doc.split() for doc in corpus_test]\n",
    "\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc_clean)]\n",
    "    model = Doc2Vec(documents, vector_size = dim, window=4, min_count=1, workers=4)\n",
    "\n",
    "    for i in range (0, len(doc_clean)):\n",
    "        trainingMatrix = np.append(trainingMatrix, [model.infer_vector(doc_clean[i])], axis=0)\n",
    "    for i in range (0, len(doc_Test)):\n",
    "        testMatrix = np.append(testMatrix, [model.infer_vector(doc_Test[i])], axis=0)\n",
    "\n",
    "    return (trainingMatrix), (testMatrix)\n",
    "\n",
    "def feature_extraction(responses):\n",
    "    featurized_responses = []\n",
    "    for response in responses:\n",
    "        features = []\n",
    "        features.append(sum(1 for word, pos in pos_tag(word_tokenize(response)) if pos.startswith('NN')))\n",
    "        features.append(sum(1 for word, pos in pos_tag(word_tokenize(response)) if pos.startswith('VB')))\n",
    "        features.append(sum(1 for word, pos in pos_tag(word_tokenize(response)) if pos.startswith('MD')))\n",
    "        features.append(sum(1 for word, pos in pos_tag(word_tokenize(response)) if pos.startswith('JJ')))\n",
    "        features.append(sum(1 for word, pos in pos_tag(word_tokenize(response)) if pos.startswith('DT')))    \n",
    "        features.append(sum(1 for word, pos in pos_tag(word_tokenize(response)) if pos.startswith('RB')))\n",
    "        features.append(len(word_tokenize(response))) \n",
    "        features.append(len(re.sub(\"\\s*\",\"\", response)))\n",
    "        features.append(len([char for char in response if char in set(string.punctuation)]))\n",
    "        featurized_responses.append(features)\n",
    "    return features\n",
    "\n",
    "def Create_w2v_matrices(df, data_type):\n",
    "    def docAveraging(sent, WV, dim):\n",
    "        summ = [0.0] * (dim)\n",
    "        A = 0.0;\n",
    "        sent_A = (re.sub(r\"[\\n(\\[\\])]\", \"\", sent)).split(\" \")\n",
    "        for word in sent_A:\n",
    "            if word in WV : #and word not in stop:\n",
    "                A = A + 1.0\n",
    "                summ = np.add(summ, WV[word])\n",
    "        if A != 0:\n",
    "            summ = summ /A\n",
    "        return summ;\n",
    "\n",
    "    if data_type == 'glove':\n",
    "        dim = [50, 100, 200, 300]\n",
    "    else:\n",
    "        dim = [300]\n",
    "    for i in range(len(df)): \n",
    "        cur_data = Data(df.file[i], df.relative_path[i])\n",
    "        #Create w2v average matrices\n",
    "        for wvsize in dim:\n",
    "            if data_type == 'glove':\n",
    "                file_2 = create_path('glove_dic','wv_dic_{}.npy'.format(wvsize))\n",
    "                WV = np.load(file_2).item() \n",
    "            elif data_type == 'cc':\n",
    "                file_2 = create_path('crawl-{}d-2M'.format(wvsize), 'crawl-{}d-2M.vec'.format(wvsize))\n",
    "                WV = load_vectors(file_2)\n",
    "            elif data_type == 'wiki':\n",
    "                file_2 = create_path('wiki-news-{}d-1M'.format(dim), 'wiki-news-{}d-1M.vec'.format(wvsize))\n",
    "                WV = load_vectors(file_2)\n",
    "            ttMatrix = np.zeros((0, wvsize))\n",
    "            print('Current word vector size: {}'.format(wvsize))\n",
    "            print('Current question: {} {}'.format(cur_data.mat_type, cur_data.quest_num))\n",
    "            for train_doc in cur_data.X:\n",
    "                ttMatrix = np.append(ttMatrix, [np.asarray(docAveraging(train_doc, WV, wvsize))], axis=0)#.decode('utf8').strip()), WV, dim))], axis=0)\n",
    "            file_3 = create_path('{}_w2v_matrices'.format(data_type),'Question{}{}_{}dimensions.npy'.format(cur_data.quest_num,cur_data.mat_type,wvsize))\n",
    "            np.save(file_3, ttMatrix) \n",
    "            save_labels(cur_data.Y, create_path(df.relative_path[i], '{}labels_question{}'.format(cur_data.mat_type, cur_data.quest_num)))   \n",
    "\n",
    "# Create_w2v_matrices(df, 'glove')\n",
    "# Create_w2v_matrices(df, 'wiki')\n",
    "# Create_w2v_matrices(df, 'cc')\n",
    "# Create_TFIDF_matrices(df, True)\n",
    "# Create_TFIDF_matrices(df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
