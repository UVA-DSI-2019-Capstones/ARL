{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_files is given a directory. It creates a database containing the name and relative path for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " def find_files(subfolder_path):\n",
    "    #This code creates a database with every file and its path\n",
    "    dir = Path(os.getcwd())\n",
    "    #Add directory where your files are:\n",
    "    newdir = dir / subfolder_path\n",
    "\n",
    "\n",
    "    #subfolders = os.listdir(newdir)\n",
    "    subfolders = ['Train_data', 'Test_data']\n",
    "    dense_list = [os.listdir(newdir / subfolder) for subfolder in subfolders]\n",
    "    paired_list = zip(dense_list, subfolders)\n",
    "\n",
    "    audio_files = [(item, label) for sublist, label in paired_list for item in sublist]\n",
    "    audio_file_list, path_list = zip(*audio_files)\n",
    "    columns = ['file', 'relative_path']\n",
    "\n",
    "    common_prefix = os.path.commonprefix([dir, newdir])\n",
    "    relative_path = os.path.relpath(newdir, common_prefix)\n",
    "    print(relative_path)\n",
    "\n",
    "    #Using os.join.path and Path() leads to a Windows path, so I had to do it this way\n",
    "    relative_path = [relative_path + '/' + path for path in path_list]\n",
    "    print(relative_path)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    print(audio_file_list)\n",
    "    print(relative_path)\n",
    "    df.file = audio_file_list\n",
    "    df.relative_path = relative_path\n",
    "    return df\n",
    "\n",
    "# df.to_csv(dir / 'audio_database.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_representations\n",
      "['Feature_representations/Train_data', 'Feature_representations/Train_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data']\n",
      "('.ipynb_checkpoints', 'Train_1.csv', 'Test_1.csv', 'Test_10.csv', 'Test_11.csv', 'Test_12.csv', 'Test_13.csv', 'Test_14.csv', 'Test_15.csv', 'Test_2.csv', 'Test_3.csv', 'Test_4.csv', 'Test_5.csv', 'Test_6.csv', 'Test_7.csv', 'Test_8.csv', 'Test_9.csv')\n",
      "['Feature_representations/Train_data', 'Feature_representations/Train_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data', 'Feature_representations/Test_data']\n"
     ]
    }
   ],
   "source": [
    "df = find_files('Feature_representations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'test' in df['file'] or 'train' in df['file']\n",
    "non_ind = list(map(lambda x: 'test' in x or 'train' in x, list(df['file'].apply(lambda x: x.lower()))))\n",
    "df = df.drop([i for i, x in enumerate(non_ind) if not x])\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           file                       relative_path         test        train\n",
      "0   Train_1.csv  Feature_representations/Train_data          NaN  Train_1.csv\n",
      "1    Test_1.csv   Feature_representations/Test_data   Test_1.csv          NaN\n",
      "2   Test_10.csv   Feature_representations/Test_data  Test_10.csv          NaN\n",
      "3   Test_11.csv   Feature_representations/Test_data  Test_11.csv          NaN\n",
      "4   Test_12.csv   Feature_representations/Test_data  Test_12.csv          NaN\n",
      "5   Test_13.csv   Feature_representations/Test_data  Test_13.csv          NaN\n",
      "6   Test_14.csv   Feature_representations/Test_data  Test_14.csv          NaN\n",
      "7   Test_15.csv   Feature_representations/Test_data  Test_15.csv          NaN\n",
      "8    Test_2.csv   Feature_representations/Test_data   Test_2.csv          NaN\n",
      "9    Test_3.csv   Feature_representations/Test_data   Test_3.csv          NaN\n",
      "10   Test_4.csv   Feature_representations/Test_data   Test_4.csv          NaN\n",
      "11   Test_5.csv   Feature_representations/Test_data   Test_5.csv          NaN\n",
      "12   Test_6.csv   Feature_representations/Test_data   Test_6.csv          NaN\n",
      "13   Test_7.csv   Feature_representations/Test_data   Test_7.csv          NaN\n",
      "14   Test_8.csv   Feature_representations/Test_data   Test_8.csv          NaN\n",
      "15   Test_9.csv   Feature_representations/Test_data   Test_9.csv          NaN\n"
     ]
    }
   ],
   "source": [
    "test_ind = list(map(lambda x: 'test' in x, list(df['file'].apply(lambda x: x.lower()))))\n",
    "df['test'] = df['file'][[i for i, x in enumerate(test_ind) if x]]\n",
    "df['train'] = df['file'][[i for i, x in enumerate(test_ind) if not x]]\n",
    "# df.drop(['file'], axis = 1, inplace = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loadGloveData takes the dimension of the Glove word vector as input. In creates a numpy version of the word vectors that is saved to the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveData(cur_dim):\n",
    "\n",
    "    #%% Set path to Glove word vector folder\n",
    "    dir = Path(os.getcwd())\n",
    "    wvpack = \"glove.6B.\"+str(cur_dim)+\"d.txt\"\n",
    "    file_1 = dir / \"glove.6B\" / wvpack\n",
    "\n",
    "    df = pd.read_csv(file_1, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    WV = {key: val.values for key, val in df.T.items()}\n",
    "    file_2 = os.path.join(dir,'glove_dic','wv_dic_{}.npy'.format(cur_dim))\n",
    "    np.save(file_2, WV) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\Documents\\GitHub\\Data Pipeline\\Test_data\n"
     ]
    }
   ],
   "source": [
    "dim = [50, 100, 200, 300]\n",
    "dir = Path(os.getcwd())\n",
    "print(dir)\n",
    "\n",
    "for example in dim:\n",
    "    fname = os.path.join(dir,'glove_dic','wv_dic_{}.npy'.format(example))\n",
    "    if not os.path.isfile(fname):\n",
    "        print(example)\n",
    "        loadGloveData(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create_train_test_w2v_matrices takes the dataframe containing the path to each train and test csv file, which is output by find_files. This function completes the processing required to create the word averaged representation of each piece of input data. The train and test sections coerce the input dataframe into the correct format, and the input is then feed to the docAveraging function. The final matrix representation is saved to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(*args):\n",
    "    cur_path = os.getcwd()\n",
    "    for value in args:\n",
    "        cur_path  = os.path.join(cur_path, value)\n",
    "    return cur_path\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    def __init__(self, file, rel_path):\n",
    "        self.quest_num = int(re.search(r'\\d+', file).group())\n",
    "        if 'test' in file.lower():\n",
    "            self.mat_type = 'test'\n",
    "            #Test Set\n",
    "            test = pd.read_csv(create_path(rel_path, file))\n",
    "            column_names = test.iloc[0,:]\n",
    "            test.drop([0], inplace=True)\n",
    "            test.rename(columns = column_names, inplace=True)\n",
    "            label_col_bound = len([type(x) for x in test.columns.values if isinstance(x, str)])\n",
    "            test = test.iloc[:, :label_col_bound]\n",
    "            test['labels_1'] = test[['Label-1']].apply(lambda x: ''.join(x.astype(str)),axis=1)\n",
    "            test['labels_2'] = test[['Label-2']].apply(lambda x: ''.join(x.astype(str)),axis=1)\n",
    "            test = test[~test['labels_1'].str.contains(\"e\")]\n",
    "            test = test[~test['labels_2'].str.contains(\"e\")]\n",
    "            self.X = test['Dialogue'].apply(lambda x : x.lower().translate(Data.table))\n",
    "            self.Y = test['labels_2']\n",
    "        #Train set\n",
    "        elif 'train' in file.lower():\n",
    "            self.mat_type = 'train'\n",
    "            #Train set\n",
    "            train = pd.read_csv(create_path(rel_path, file))\n",
    "            train.drop([train.columns[0]], axis = 1, inplace=True)\n",
    "            train.dropna(inplace=True)\n",
    "            train['labels']=train.labels.apply(lambda x: ''.join([(3-len(str(x)))*'0',str(x)]))\n",
    "            self.X = train['Dialogue'].apply(lambda x : x.lower().translate(Data.table))\n",
    "            self.Y = train['labels']\n",
    "            \n",
    "def save_labels(Y, out_path):\n",
    "    if not os.path.isfile(out_path):\n",
    "        np.save(out_path, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    #Map POS tag to first character lemmatize() accepts\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def Create_TFIDF_matrices(df, lemmatize = False):\n",
    "    lemtz = ''\n",
    "    if lemmatize == True:\n",
    "        lemtz = 'lemmatized'      \n",
    "    test_df = df[['test', 'relative_path']].dropna()\n",
    "    train_df = df[['train', 'relative_path']].dropna()\n",
    "    test_df.reset_index(inplace = True)\n",
    "    train_df.reset_index(inplace = True)\n",
    "    if len(test_df) <= len(train_df):\n",
    "        for i in range(min([len(test_df), len(train_df)])):  \n",
    "            train_data = Data(train_df.train[i], train_df.relative_path[i])\n",
    "# Attributes of train_data:   mat_type, quest_num, X_train, Y_train \n",
    "            #Sanity Check\n",
    "            if train_data.mat_type != 'train' or train_data.quest_num != str(i+1):\n",
    "                print('Expected matrix type train, received type {}'.format(train_data.mat_type))\n",
    "                print('Expected question # {}, received # {}'.format(i+1, train_data.quest_num))\n",
    "                print('error')\n",
    "            test_data = Data(test_df.test[i], test_df.relative_path[i])\n",
    "            #Sanity Check\n",
    "            if test_data.mat_type != 'test' or test_data.quest_num != str(i+1):\n",
    "                print('Expected matrix type test, received type {}'.format(test_data.mat_type))\n",
    "                print('Expected question # {}, received # {}'.format(i+1, test_data.quest_num))\n",
    "                print('error')\n",
    "            Y_train = train_data.Y\n",
    "            Y_test = test_data.Y\n",
    "            #Create TF-IDF Matrices (we want to tokenize the text before creating these)\n",
    "            if lemmatize==True: \n",
    "                lemmatizer =WordNetLemmatizer() \n",
    "                X_test = []\n",
    "                X_train = []\n",
    "                for sent in test_data.X:\n",
    "                    X_test.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])) \n",
    "                for sent in train_data.X:\n",
    "                    X_train.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])) \n",
    "            X_test = test_data.X\n",
    "            X_train = train_data.X\n",
    "#             #Flatten lists\n",
    "#             X_train = [item for sublist in X_train for item in sublist]\n",
    "#             X_test = [item for sublist in X_test for item in sublist]\n",
    "            print('Length of X_train: ' + str(len(X_train)))\n",
    "            print('Length of X_test: ' + str(len(X_test)))\n",
    "            for dim in range (100, 200, 100): #max 2100\n",
    "                tfidf_vectorizer = TfidfVectorizer(max_features = dim)\n",
    "                tfidf_matrix = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "                tfidf_matrix_Test = tfidf_vectorizer.transform(X_test).toarray()\n",
    "                file_4 = create_path('tfidf_matrices','TFIDF_train_{}Question{}_{}dim.npy'.format(lemtz, train_data.quest_num, dim))\n",
    "                file_5 = create_path('tfidf_matrices','TFIDF_test_{}Question{}_{}dim.npy'.format(lemtz, test_data.quest_num, dim))\n",
    "                np.save(file_4, tfidf_matrix)\n",
    "                np.save(file_5, tfidf_matrix_Test)\n",
    "            save_labels(Y_train, create_path('labels', '{}_labels_question{}'.format(train_data.mat_type, train_data.quest_num)))\n",
    "            save_labels(Y_test, create_path('labels', '{}_labels_question{}'.format(test_data.mat_type, test_data.quest_num)))\n",
    "\n",
    "def Create_w2v_matrices(df, data_type):\n",
    "    def docAveraging(sent, WV, dim):\n",
    "        summ = [0.0] * (dim)\n",
    "        A = 0.0;\n",
    "        sent_A = (re.sub(r\"[\\n(\\[\\])]\", \"\", sent)).split(\" \")\n",
    "        for word in sent_A:\n",
    "            if word in WV : #and word not in stop:\n",
    "                A = A + 1.0\n",
    "                for i in range(0, dim):\n",
    "                    summ[i] = summ[i] + float((WV[word])[i])\n",
    "        if A != 0:\n",
    "            #A = 1\n",
    "            for i in range(0, dim):\n",
    "                summ[i] = summ[i] / A\n",
    "        return summ;\n",
    "    if data_type == 'glove':\n",
    "        dim = [50, 100, 200, 300]\n",
    "    else:\n",
    "        dim = [300]\n",
    "    for i in range(len(df)): \n",
    "        cur_data = Data(df.file[i], df.relative_path[i])\n",
    "        #Create w2v average matrices\n",
    "        for wvsize in dim:\n",
    "            if data_type == 'glove':\n",
    "                file_2 = create_path('glove_dic','wv_dic_{}.npy'.format(wvsize))\n",
    "                WV = np.load(file_2).item() \n",
    "            elif data_type == 'cc':\n",
    "                file_2 = create_path('crawl-{}d-2M'.format(wvsize), 'crawl-{}d-2M.vec'.format(wvsize))\n",
    "                WV = load_vectors(file_2)\n",
    "            elif data_type == 'wiki':\n",
    "                file_2 = create_path('wiki-news-{}d-1M'.format(dim), 'wiki-news-{}d-1M.vec'.format(wvsize))\n",
    "                WV = load_vectors(file_2)\n",
    "            ttMatrix = np.zeros((0, wvsize))\n",
    "            print('Current word vector size: {}'.format(wvsize))\n",
    "            print('Current question: {} {}'.format(cur_data.mat_type, cur_data.quest_num))\n",
    "            for train_doc in cur_data.X:\n",
    "                ttMatrix = np.append(ttMatrix, [np.asarray(docAveraging(train_doc, WV, wvsize))], axis=0)#.decode('utf8').strip()), WV, dim))], axis=0)\n",
    "            file_3 = create_path('{}_w2v_matrices'.format(data_type),'Question{}{}_{}dimensions.npy'.format(cur_data.quest_num,cur_data.mat_type,wvsize))\n",
    "            np.save(file_3, ttMatrix) \n",
    "            save_labels(cur_data.Y, create_path(df.relative_path[i], '{}labels_question{}'.format(cur_data.mat_type, cur_data.quest_num)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    Test_3.csv\n",
       "Name: file, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df[df['file'] == 'Test_3.csv']\n",
    "df.reset_index(inplace = True)\n",
    "# Create_w2v_matrices(df, 'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The strip bat be hang on their foot for best',\n",
       " 'this be another test sentence',\n",
       " 'we have so many test sentence']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = [\"The striped bats are hanging on their feet for best\", 'this is another testing sentence', 'we have so many test sentences']\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "X_example = []\n",
    "for sent in test_string:\n",
    "    X_example.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)]))\n",
    "#     lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "X_example#     print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bat'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatizer.lemmatize('bats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.174350e-01,  3.112450e-01, -3.956750e-01, -5.039500e-02,\n",
       "        6.477200e-01, -6.878950e-01, -8.815050e-01,  3.829150e-01,\n",
       "       -1.835915e-01, -1.167050e-01, -3.328950e-01, -2.313550e-01,\n",
       "       -3.114600e-01,  1.462275e-01,  4.800850e-01,  1.795145e-01,\n",
       "       -2.757800e-01, -5.430000e-03, -6.132382e-01, -6.150800e-01,\n",
       "        1.777250e-01,  9.637950e-01,  3.983100e-01, -7.735000e-04,\n",
       "        7.660700e-01, -1.366250e+00, -2.217800e-01,  5.830750e-01,\n",
       "        6.137100e-01, -1.183950e-01,  3.441950e+00,  6.414950e-01,\n",
       "       -1.485450e-01,  1.750625e-01,  1.220980e-01, -2.208225e-01,\n",
       "        1.286050e-01,  2.898450e-01,  2.893150e-01, -1.923750e-01,\n",
       "       -2.414500e-01,  1.828941e-01, -3.540550e-01, -5.653000e-02,\n",
       "        3.748100e-01,  3.073900e-02, -5.536000e-03,  3.422500e-02,\n",
       "        3.665200e-01,  5.917200e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " file_2 = create_path('glove_w2v_matrices','Question1test_50dimensions.npy')\n",
    "WV = np.load(file_2)\n",
    "WV[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
