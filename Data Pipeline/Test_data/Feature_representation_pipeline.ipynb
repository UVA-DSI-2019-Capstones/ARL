{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_files is given a directory. It creates a database containing the name and relative path for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " def find_files(subfolder_path):\n",
    "    #This code creates a database with every file and its path\n",
    "    dir = Path(os.getcwd())\n",
    "    #Add directory where your files are:\n",
    "    newdir = dir / subfolder_path\n",
    "\n",
    "\n",
    "    #subfolders = os.listdir(newdir)\n",
    "    subfolders = ['Train_data', 'Test_data']\n",
    "    dense_list = [os.listdir(newdir / subfolder) for subfolder in subfolders]\n",
    "    paired_list = zip(dense_list, subfolders)\n",
    "\n",
    "    audio_files = [(item, label) for sublist, label in paired_list for item in sublist]\n",
    "    audio_file_list, path_list = zip(*audio_files)\n",
    "    columns = ['file', 'relative_path']\n",
    "\n",
    "    common_prefix = os.path.commonprefix([dir, newdir])\n",
    "    relative_path = os.path.relpath(newdir, common_prefix)\n",
    "    print(relative_path)\n",
    "\n",
    "    #Using os.join.path and Path() leads to a Windows path, so I had to do it this way\n",
    "    relative_path = [relative_path + '/' + path for path in path_list]\n",
    "    print(relative_path)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    print(audio_file_list)\n",
    "    print(relative_path)\n",
    "    df.file = audio_file_list\n",
    "    df.relative_path = relative_path\n",
    "    return df\n",
    "\n",
    "# df.to_csv(dir / 'audio_database.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_representations\n",
      "['Feature_representations/Train_data', 'Feature_representations/Test_data']\n",
      "('Train_1.csv', 'Test_1.csv')\n",
      "['Feature_representations/Train_data', 'Feature_representations/Test_data']\n"
     ]
    }
   ],
   "source": [
    "df = find_files('Feature_representations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          file                       relative_path        test        train\n",
      "0  Train_1.csv  Feature_representations/Train_data         NaN  Train_1.csv\n",
      "1   Test_1.csv   Feature_representations/Test_data  Test_1.csv          NaN\n"
     ]
    }
   ],
   "source": [
    "test_ind = list(map(lambda x: 'test' in x, list(df['file'].apply(lambda x: x.lower()))))\n",
    "df['test'] = df['file'][[i for i, x in enumerate(test_ind) if x]]\n",
    "df['train'] = df['file'][[i for i, x in enumerate(test_ind) if not x]]\n",
    "# df.drop(['file'], axis = 1, inplace = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loadGloveData takes the dimension of the Glove word vector as input. In creates a numpy version of the word vectors that is saved to the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveData(cur_dim):\n",
    "\n",
    "    #%% Set path to Glove word vector folder\n",
    "    dir = Path(os.getcwd())\n",
    "    wvpack = \"glove.6B.\"+str(cur_dim)+\"d.txt\"\n",
    "    file_1 = dir / \"glove.6B\" / wvpack\n",
    "\n",
    "    df = pd.read_csv(file_1, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    WV = {key: val.values for key, val in df.T.items()}\n",
    "    file_2 = os.path.join(dir,'glove_dic','wv_dic_{}.npy'.format(cur_dim))\n",
    "    np.save(file_2, WV) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\Documents\\GitHub\\Data Pipeline\\Test_data\n"
     ]
    }
   ],
   "source": [
    "dim = [50, 100, 200, 300]\n",
    "dir = Path(os.getcwd())\n",
    "print(dir)\n",
    "\n",
    "for example in dim:\n",
    "    fname = os.path.join(dir,'glove_dic','wv_dic_{}.npy'.format(example))\n",
    "    if not os.path.isfile(fname):\n",
    "        print(example)\n",
    "        loadGloveData(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create_train_test_w2v_matrices takes the dataframe containing the path to each train and test csv file, which is output by find_files. This function completes the processing required to create the word averaged representation of each piece of input data. The train and test sections coerce the input dataframe into the correct format, and the input is then feed to the docAveraging function. The final matrix representation is saved to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(*args):\n",
    "    cur_path = os.getcwd()\n",
    "    for value in args:\n",
    "        cur_path  = os.path.join(cur_path, value)\n",
    "    return cur_path\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    def __init__(self, file, rel_path):\n",
    "        self.quest_num = file[-5]\n",
    "        if 'test' in file.lower():\n",
    "            self.mat_type = 'test'\n",
    "            #Test Set\n",
    "            test = pd.read_csv(create_path(rel_path, file))\n",
    "            column_names = test.iloc[0,:]\n",
    "            test.drop([0], inplace=True)\n",
    "            test.rename(columns = column_names, inplace=True)\n",
    "            test.drop(columns = ['Section'], inplace=True)\n",
    "            test.dropna(inplace=True)\n",
    "            test['labels'] = test[test.columns[1:]].apply(\n",
    "                lambda x: ''.join(x.astype(str)),axis=1)\n",
    "            test.drop(column_names[2:], axis=1, inplace=True)\n",
    "            test.drop(test[test.labels=='eee'].index, inplace=True)\n",
    "            self.X = test['Dialogue'].apply(lambda x : x.lower().translate(Data.table))\n",
    "            self.Y = test['labels']\n",
    "        #Train set\n",
    "        elif 'train' in file.lower():\n",
    "            self.mat_type = 'train'\n",
    "            #Train set\n",
    "            train = pd.read_csv(create_path(rel_path, file))\n",
    "            train.drop([train.columns[0]], axis = 1, inplace=True)\n",
    "            train.dropna(inplace=True)\n",
    "            train['labels']=train.labels.apply(lambda x: ''.join([(3-len(str(x)))*'0',str(x)]))\n",
    "            self.X = train['Dialogue'].apply(lambda x : x.lower().translate(Data.table))\n",
    "            self.Y = train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labels(Y, out_path):\n",
    "    if not os.path.isfile(out_path):\n",
    "        np.save(out_path, Y)\n",
    "\n",
    "def Create_TFIDF_matrices(df):\n",
    "    test_df = df[['test', 'relative_path']].dropna()\n",
    "    train_df = df[['train', 'relative_path']].dropna()\n",
    "    test_df.reset_index(inplace = True)\n",
    "    train_df.reset_index(inplace = True)\n",
    "    if len(test_df) <= len(train_df):\n",
    "        for i in range(min([len(test_df), len(train_df)])):  \n",
    "            train_data = Data(train_df.train[i], train_df.relative_path[i])\n",
    "# Attributes of train_data:   mat_type, quest_num, X_train, Y_train \n",
    "            #Sanity Check\n",
    "            if train_data.mat_type != 'train' or train_data.quest_num != str(i+1):\n",
    "                print('Expected matrix type train, received type {}'.format(train_data.mat_type))\n",
    "                print('Expected question # {}, received # {}'.format(i+1, train_data.quest_num))\n",
    "                print('error')\n",
    "            test_data = Data(test_df.test[i], test_df.relative_path[i])\n",
    "            #Sanity Check\n",
    "            if test_data.mat_type != 'test' or test_data.quest_num != str(i+1):\n",
    "                print('Expected matrix type test, received type {}'.format(test_data.mat_type))\n",
    "                print('Expected question # {}, received # {}'.format(i+1, test_data.quest_num))\n",
    "                print('error')\n",
    "            Y_train = train_data.Y\n",
    "            Y_test = test_data.Y\n",
    "            #Create TF-IDF Matrices (we want to tokenize the text before creating these)\n",
    "            X_train = train_data.X.apply(lambda x: word_tokenize(x))\n",
    "            X_test = test_data.X.apply(lambda x: word_tokenize(x))\n",
    "            #Flatten lists\n",
    "            X_train = [item for sublist in X_train for item in sublist]\n",
    "            X_test = [item for sublist in X_test for item in sublist]\n",
    "            for dim in range (100, 200, 100): #max 2100\n",
    "                tfidf_vectorizer = TfidfVectorizer(max_features = dim)\n",
    "                tfidf_matrix = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "                tfidf_matrix_Test = tfidf_vectorizer.transform(X_test).toarray()\n",
    "                file_4 = create_path('tfidf_matrices','TFIDF_train_Question{}_{}dim.npy'.format(train_data.quest_num, dim))\n",
    "                file_5 = create_path('tfidf_matrices','TFIDF_test_Question{}_{}dim.npy'.format(test_data.quest_num, dim))\n",
    "                np.save(file_4, tfidf_matrix)\n",
    "                np.save(file_5, tfidf_matrix_Test)\n",
    "            save_labels(Y_train, create_path('labels', '{}_labels_question{}'.format(train_data.mat_type, train_data.quest_num)))\n",
    "            save_labels(Y_test, create_path('labels', '{}_labels_question{}'.format(test_data.mat_type, test_data.quest_num)))\n",
    "\n",
    "def Create_glove_w2v_matrices(df):\n",
    "    def docAveraging(sent, WV, dim):\n",
    "        summ = [0.0] * (dim)\n",
    "        A = 0.0;\n",
    "        sent_A = (re.sub(r\"[\\n(\\[\\])]\", \"\", sent)).split(\" \")\n",
    "        for word in sent_A:\n",
    "            if word in WV : #and word not in stop:\n",
    "                A = A + 1.0\n",
    "                for i in range(0, dim):\n",
    "                    summ[i] = summ[i] + float((WV[word])[i])\n",
    "        if A != 0:\n",
    "            #A = 1\n",
    "            for i in range(0, dim):\n",
    "                summ[i] = summ[i] / A\n",
    "        return summ;\n",
    "    \n",
    "    dim = [50, 100, 200, 300]\n",
    "    for i in range(len(df)): \n",
    "        cur_data = Data(df.file[i], df.relative_path[i])\n",
    "        #Create w2v average matrices\n",
    "        for wvsize in dim:\n",
    "            file_2 = create_path('glove_dic','wv_dic_{}.npy'.format(wvsize))\n",
    "            WV = np.load(file_2).item() \n",
    "            ttMatrix = np.zeros((0, wvsize))\n",
    "            print('Current word vector size: {}'.format(wvsize))\n",
    "            print('Current question: {} {}'.format(cur_data.mat_type, cur_data.quest_num))\n",
    "            for train_doc in cur_data.X:\n",
    "                ttMatrix = np.append(ttMatrix, [np.asarray(docAveraging(train_doc, WV, wvsize))], axis=0)#.decode('utf8').strip()), WV, dim))], axis=0)\n",
    "            file_3 = create_path('w2v_matrices','Question{}{}_{}dimensions.npy'.format(cur_data.quest_num,cur_data.mat_type,wvsize))\n",
    "            np.save(file_3, ttMatrix) \n",
    "            save_labels(cur_data.Y, create_path(df.relative_path[i], '{}labels_question{}'.format(cur_data.mat_type, cur_data.quest_num)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current word vector size: 50\n",
      "Current question: train 1\n",
      "Current word vector size: 100\n",
      "Current question: train 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-3636573d2591>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCreate_glove_w2v_matrices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-97-0814a90a3c24>\u001b[0m in \u001b[0;36mCreate_glove_w2v_matrices\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current question: {} {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquest_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtrain_doc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[0mttMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mttMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocAveraging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwvsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.decode('utf8').strip()), WV, dim))], axis=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mfile_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w2v_matrices'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Question{}{}_{}dimensions.npy'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquest_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcur_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwvsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttMatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4526\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4527\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4528\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Create_glove_w2v_matrices(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
