{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan 22 14:20:54 2019\n",
    "\n",
    "@author: Erfaneh\n",
    "\"\"\"\n",
    "#%% imports\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score as classification_report\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#%% colors for t-sne plot\n",
    "\n",
    "colors = ['green', 'gold', 'crimson', 'darkblue', 'orangered', 'orchid', 'olive', 'dodgerblue', 'yellowgreen', 'lime', 'salmon', 'darkseagreen', 'wheat']\n",
    "classes = [\"1.00\", \"1.33\", \"1.67\", \"2.00\", \"2.33\", \"2.67\", \"3.00\", \"3.33\", \"3.67\", \"4.00\", \"4.33\", \"4.67\", \"5.00\"]\n",
    "labelColor = {}\n",
    "for i in range(0, len(classes)):\n",
    "    labelColor[classes[i]] = colors[i]\n",
    "\n",
    "#%% Functions\n",
    "def Extractor(text, POS):\n",
    "    nouns = []\n",
    "    text2 = nlp(text)\n",
    "    for word in text2:\n",
    "        if(word.pos_ == POS):\n",
    "            nouns.append(str(word))\n",
    "    #print(len(nouns))\n",
    "    return \" \".join(nouns)\n",
    "\n",
    "def FilterPunc(doc):\n",
    "    exclude = set(string.punctuation)\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in doc if ch not in exclude)\n",
    "    #normalized = \"\".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return punc_free\n",
    "\n",
    "def syn_DicCreator (X_train, POS):\n",
    "    syns_Dic = {}\n",
    "    for text in X_train:\n",
    "        words = Extractor(FilterPunc(text).lower(), POS).split(\" \")\n",
    "        for word in words:\n",
    "            if word not in syns_Dic:\n",
    "                syns = wordnet.synsets(word, pos = POS[0].lower())\n",
    "                syns_Name = list(set([synset.lemmas()[0].name() for synset in syns]))\n",
    "                if word in syns_Name:\n",
    "                    syns_Name.remove(word) \n",
    "                if (len(syns_Name)!= 0):\n",
    "                    syns_Dic[word] = syns_Name\n",
    "    return syns_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import os\n",
    "dir = os.getcwd()\n",
    "\n",
    "dim = 200\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#%%\n",
    "#Get working directory\n",
    "dir = Path(os.getcwd())\n",
    "#Move up two directories\n",
    "# p = Path(dir).parents[1]\n",
    "print(dir)\n",
    "\n",
    "#%%\n",
    "#Open the word vectors:\n",
    "WV = {}\n",
    "wvpack = \"glove.6B.\"+str(dim)+\"d.txt\"\n",
    "file_1 = dir / \"glove.6B\" / wvpack\n",
    "#%%\n",
    "df = pd.read_csv(file_1, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "print(df.head())\n",
    "WV = {key: val.values for key, val in df.T.items()}\n",
    "\n",
    "\n",
    "#%% Read Data\n",
    "train_df = pd.read_csv(dir / 'train_set.csv', delimiter = ',')\n",
    "test_df = pd.read_csv(dir / 'test_set.csv', delimiter = ',')\n",
    "# df = df.fillna(\" \")\n",
    "\n",
    "#%% \n",
    "print(train_df.columns)\n",
    "X_train = train_df['response_text']\n",
    "Y_train = train_df['response_round_score']\n",
    "X_test = test_df['response_text']\n",
    "Y_test = test_df['response_round_score']\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# #%% Dictionaries of Synonyms\n",
    "# synDic_Noun = syn_DicCreator(X_train, 'NOUN')\n",
    "# synDic_Verb = syn_DicCreator(X_train, 'VERB')\n",
    "# synDic_Adj = syn_DicCreator(X_train, 'ADJ')\n",
    "\n",
    "#%%\n",
    "import numpy as np\n",
    "\n",
    "# # Save\n",
    "# np.save('synDic_Noun.npy', synDic_Noun) \n",
    "# np.save('synDic_Verb.npy', synDic_Verb)\n",
    "# np.save('synDic_Adj.npy', synDic_Adj)\n",
    "\n",
    "#%%\n",
    "synDic_Noun = np.load('synDic_Noun.npy').item()\n",
    "synDic_Verb = np.load('synDic_Verb.npy').item()\n",
    "synDic_Adj = np.load('synDic_Adj.npy').item()\n",
    "\n",
    "#%% Data resampling\n",
    "new_X = []\n",
    "new_Y = []\n",
    "similarity_level = []\n",
    "\n",
    "for i in range(0, len(X_train)):\n",
    "    text = FilterPunc(X_train[i]).lower().replace(\"\\\"\", \"\")\n",
    "    new_X.append(text)\n",
    "    new_Y.append(Y_train[i])\n",
    "    words = nlp(text)\n",
    "    for word in words:\n",
    "        if(word.pos_ == 'NOUN'): \n",
    "            word = str(word)\n",
    "            if (word in synDic_Noun.keys()):\n",
    "                for syn in synDic_Noun[word]:\n",
    "                    syn_token = nlp(syn)\n",
    "                    word_token = nlp(word)\n",
    "                    similarity_level.append(word_token.similarity(syn_token))\n",
    "                    new_X.append(text.replace(word, syn))\n",
    "                    new_Y.append(Y_train[i])                    \n",
    "            continue\n",
    "        if(word.pos_ == 'VERB'):\n",
    "            word = str(word)\n",
    "            if word in synDic_Verb:\n",
    "                for syn in synDic_Verb[word]:\n",
    "                    syn_token = nlp(syn)\n",
    "                    word_token = nlp(word)\n",
    "                    similarity_level.append(word_token.similarity(syn_token))\n",
    "                    new_X.append(text.replace(word, syn))\n",
    "                    new_Y.append(Y_train[i])\n",
    "            continue\n",
    "        if(word.pos_ == 'ADJ'):\n",
    "            word = str(word)\n",
    "            if word in synDic_Verb:\n",
    "                for syn in synDic_Verb[word]:\n",
    "                    syn_token = nlp(syn)\n",
    "                    word_token = nlp(word)\n",
    "                    similarity_level.append(word_token.similarity(syn_token))\n",
    "                    new_X.append(text.replace(word, syn))\n",
    "                    new_Y.append(Y_train[i])  \n",
    "\n",
    "X_train_new = new_X\n",
    "Y_train_new = new_Y\n",
    "\n",
    "#%%\n",
    "df = pd.DataFrame([X_train_new, Y_train_new, similarity_level]).transpose()\n",
    "print(df.head())\n",
    "\n",
    "#%%\n",
    "df.to_csv('rephrasings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-gpu",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
